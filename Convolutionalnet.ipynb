{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfvHwdBAY9_G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "path = './'\n",
        "os.makedirs(os.path.join(path, 'Convolutionalnet', 'data'), exist_ok=True)\n",
        "root_dir = os.path.join(path, 'Convolutionalnet')\n",
        "\n",
        "\n",
        "!pip3 install --upgrade gdown --quiet\n",
        "!gdown 1z3B1GR7UtHZGrqNjUaep6thHwrN3IYSI\n",
        "\n",
        "tar = tarfile.open(\"data.tar.gz\", \"r:gz\")\n",
        "total_size = sum(f.size for f in tar.getmembers())\n",
        "with tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Extracting tar.gz file\") as pbar:\n",
        "    for member in tar.getmembers():\n",
        "        tar.extract(member, os.path.join(root_dir, 'data'))\n",
        "        pbar.update(member.size)\n",
        "tar.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdyI2ambA8Z_",
        "outputId": "2c2d3275-e0ca-47f3-8445-556eba92b8d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1z3B1GR7UtHZGrqNjUaep6thHwrN3IYSI\n",
            "From (redirected): https://drive.google.com/uc?id=1z3B1GR7UtHZGrqNjUaep6thHwrN3IYSI&confirm=t&uuid=685706d2-803c-4552-b221-99bae2dd2d23\n",
            "To: /content/data.tar.gz\n",
            "100% 423M/423M [00:11<00:00, 36.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting tar.gz file: 100%|██████████| 526M/526M [00:17<00:00, 29.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "root_dir = os.path.join(path, 'Convolutionalnet/data')\n",
        "class MiniPlaces(Dataset):\n",
        "    def __init__(self, root_dir, split, transform=None, label_dict=None):\n",
        "        assert split in ['train', 'val', 'test']\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "        self.labels = []\n",
        "        self.label_dict = label_dict if label_dict is not None else {}\n",
        "        if split == \"train\" or split == \"val\":\n",
        "            with open(os.path.join(root_dir, (\"train\" if self.split == \"train\" else \"val\") + \".txt\")) as f:\n",
        "                for line in f:\n",
        "                    line = line.rstrip().split()\n",
        "                    n = int(line[0][-12:-4])\n",
        "                    if n <= 900:\n",
        "                        self.filenames.append(os.path.join(line[0]))\n",
        "                        self.labels.append(int(line[1]))\n",
        "        if label_dict is None and split == \"train\":\n",
        "            with open(os.path.join(root_dir, \"train.txt\")) as f:\n",
        "                num = -1\n",
        "                for line in f:\n",
        "                    line = line.rstrip().split()\n",
        "                    if int(line[1]) > num:\n",
        "                        num += 1\n",
        "                        self.label_dict.update({int(line[1]): line[0][8:line[0].find(\"/\", 8)]})\n",
        "        if split == \"test\":\n",
        "            self.labels = os.listdir(os.path.join(root_dir, \"images\", \"test\"))\n",
        "            self.filenames = [\"test/\" + i for i in self.labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        dataset_len = len(self.labels)\n",
        "        return dataset_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.root_dir, \"images\", self.filenames[idx]))\n",
        "        if not self.transform is None:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "miniplaces_train = MiniPlaces(data_root, split='train', transform=data_transform)\n",
        "miniplaces_val = MiniPlaces(\n",
        "    data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=miniplaces_train.label_dict)\n"
      ],
      "metadata": {
        "id": "R3o0psorbVVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "FPrbHKHwb8dq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a5cf00-b563-4406-f0ca-c6e06eaf1a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda. Good to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
        "    model = model.to(device)\n",
        "    best_acc = 0\n",
        "    flag = False\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
        "            for inputs, labels in train_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
        "        if best_acc > accuracy:\n",
        "            if flag:\n",
        "                print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
        "                break\n",
        "            else:\n",
        "                flag = True\n",
        "        else:\n",
        "            best_acc = accuracy\n",
        "            flag = False\n",
        "        print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0.0\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predictions = torch.max(logits, dim=1)\n",
        "            num_correct += (predictions == labels).sum().item()\n",
        "            num_samples += len(inputs)\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = num_correct / num_samples\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def predict(model, test_dataloader):\n",
        "    out = []\n",
        "    for i in test_dataloader:\n",
        "        pic = i[0]\n",
        "        lab = torch.argmax(model.to('cpu')(pic))\n",
        "        out.append(lab.item())\n",
        "\n",
        "    return out\n",
        "\n",
        "data_transform_flatten = transforms.Compose([data_transform, torch.flatten])"
      ],
      "metadata": {
        "id": "L6YD_oc_akI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels, conv_hidden_channels, conv_out_channels,\n",
        "        input_size=(64,64),\n",
        "        dropout_rate1=0.25, dropout_rate2=0.5,\n",
        "        fc_out_channels=128, num_classes=100,\n",
        "        kernel_size=3, stride=1, padding=1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        flatten_size = conv_out_channels * int(((input_size[0] + (2 * padding) - kernel_size) / stride) + 1) * int(((input_size[1] + (2 * padding) - kernel_size) / stride) + 1)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_channels, conv_hidden_channels, kernel_size, stride, padding)\n",
        "        self.conv2 = nn.Conv2d(conv_hidden_channels, conv_out_channels, kernel_size, stride,(padding, padding))\n",
        "        self.max_pooling = nn.MaxPool2d(kernel_size,stride, padding)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate1)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate2)\n",
        "        self.fc1 = nn.Linear(flatten_size, fc_out_channels)\n",
        "        self.fc2 = nn.Linear(fc_out_channels, num_classes)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, x, return_intermediate=False):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.max_pooling(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "PmIcfxS7ZCoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "conv_val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform)\n",
        "conv_train_loader = torch.utils.data.DataLoader(\n",
        "    conv_train_dataset, batch_size=64, num_workers=0, shuffle=True)\n",
        "conv_val_loader = torch.utils.data.DataLoader(\n",
        "    conv_val_dataset, batch_size=64, num_workers=0, shuffle=False)\n",
        "\n",
        "model = Conv(\n",
        "    input_channels=3, conv_hidden_channels=64, conv_out_channels=128,\n",
        "    input_size=(64,64),\n",
        "    dropout_rate1=0.25, dropout_rate2=0.5,\n",
        "    fc_out_channels=128,\n",
        "    kernel_size=3, stride=1, padding=1,\n",
        "    num_classes=len(miniplaces_train.label_dict))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "8PlcsiTWauqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum = 0.6)"
      ],
      "metadata": {
        "id": "8wvHc4lgLpeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, conv_train_loader, conv_val_loader, optimizer, criterion, device, num_epochs=1)"
      ],
      "metadata": {
        "id": "E1pIdU2Hax5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2602262-6b12-4194-e908-f9da05af8378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 1407/1407 [02:34<00:00,  9.13it/s, loss=2.08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 3.2352, Accuracy = 0.2300\n"
          ]
        }
      ]
    }
  ]
}